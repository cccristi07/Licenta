\chapter{Clasificarea defectelor folosind metoda învățării de dicționare rare}
\label{chap:dictionary_learning}
\section{Aspecte teoretice}
Problema învățării dicționarelor se clasează în domeniul problemelor de învățare nesupervizată - setul de date nu trebuie să fie structurat pe clase - dar există și variațiuni ale acesteia prin care modifică criteriul de optimizat astfel încât să se poată învăța dicționare specializate pentru clasificare.

Având ca date de intrare un set de date $\mathbf{Y} \in \mathbb{R}^{n \times n_s}$, unde $n$ reprezintă dimensiunea semnalelor și $n_s$ reprezintă numărul de semnale folosite la antrenare, dorim să găsim acele matrice $\mathbf{D} \in \mathbb{R}^{n \times m}$ și $\mathbf{X} \in \mathbb{R}^{m \times n_s}$ astfel încât să rezolvăm problema de optimizare\cite[Capitol 2]{DL_book}:

\begin{subequations}
\begin{alignat}{2}
&\!\min_{\mathbf{D}, \mathbf{R}}        &\qquad& ||\mathbf{Y} - \mathbf{D} \mathbf{X} ||^{2}_{F} \label{eq:dl_opt}\\
&\text{s.l.:} &      & ||x_l||_0  \leq s, l = 1:N \label{eq:dl_sparsity}\\
& & & ||d_j|| = 1, j = 1:n \label{eq:dl_norm}
\end{alignat}
\label{eq:dl_opt_problem}
\end{subequations}
Unde:
\begin{itemize}
    \item $\mathbf{D}$ reprezintă matricea dicționarului pe baza căruia se va calcula reprezentarea, coloanele acesteia se numesc atomi
    \item $\mathbf{X}$ este reprezentarea rară a setului de date $\mathbf{Y}$
\end{itemize}

Constrângerea \eqref{eq:dl_sparsity} se referă la raritatea vectorului de reprezentare iar \eqref{eq:dl_norm} la normalizarea atomilor pentru dicționar.

Problema de găsire a dicționarului și a reprezentării semnalelor de antrenare $\mathbf{Y}$ conține neliniarități puternice din cauza condiției de raritate impuse. Cazul interesant și cel mai abordat în literatură îl reprezintă acela în care dicționarul este supracomplet \cite[Capitolul 1]{DL_book}, această proprietate poate aduce numeroase beneficii diferitelor procese de clasificare, anume:
\begin{itemize}
    \item stocarea matricelor sparse se face mult mai eficient decât a celor pline
    \item din punct de vedere computațional există foarte multe multiplicări care nu se vor mai efectua
\end{itemize}

Modul în care am descris problema duce cu gândul la o metodă sofisticată de extragere a caracteristicilor din setul de date. Astfel dacă extragem o reprezentare rară $X = \{x_i\}$, putem să folosim vectorul rar $x_i$ ca exemplu de antrenare pentru alți algoritmi de antrenare sau clasificare.


\subsection{Găsirea reprezentării rare}
Reprezentarea sparsă se ocupă de reconstruirea unui semnal $y \in \mathbf{R}^n$ având la dispoziție un dicționar $\mb{D} \in \mathbb{R}^{n\times m}, \mb{D} = [d_1, d_2, ..., d_m]$ cu $m > n$, adică, găsirea unui vector $\mb{x} \in \mathbb{R}^m$ astfel încât $y \approx \mb{D} \mb{x}$. 
Reziduul aproximării sparse este definit ca:

\begin{equation}
    e = y - \mb{D} \mb{x}
\end{equation}

Un algoritm greedy care rezolvă această problemă este OMP \textit{Orthogonal Mathching Pursuit}. Având la un moment dat mulțimea $\mathcal{S}$ a atomilor din dicționar selectați, algoritmul dorește să atingă un anumit nivel de sparsitate $s = |\mathcal{S}|$ și îndeplinirea unui criteriu de eroare $||y - \mb{D_{\mathcal{S}}} \mb{x}|| < tol$ \cite[Capitolul 1]{DL_book}. Algoritmul va completa mulțimea $\mathcal{S}$  cu acel atom $d_k$ care nu se află în ea și care va fi cel mai bine corelat cu reziduul actual, deci $k = arg \max_{j \not \in \mathcal{S}} |e^T x_j|$. 

Pentru a putea obține reprezentarea semnalului $y$ folosind mulțimea de atomi $\mathcal{S}$ este nevoie să rezolvăm sistemul de ecuații supradeterminat:

\begin{equation}
    \mb{D} \mb{x} = \mb{y}
\end{equation}

Prin folosirea soluției ecuațiilor normale:

\begin{equation}
    \mb{x} = (\mb{D} \mb{D}^T)^{-1} \mb{D} \mb{y}
\end{equation}


\section{Adaptarea la problema rețelelor de apă}

Pentru a putea clasifica nodurile unde s-au produs defecte trebuie extinsă problema de optimizare \eqref{eq:dl_opt_problem} astfel încât să se ia în calcul o structură eficientă a dicționarului pentru reprezentarea sparsă a reziduurilor, dar și apariția unei noi matrici $\mathbf{W}$ numită clasificator \cite{DL_book}:
\begin{equation}
    \min_{\mathbf{W}} || \mathbf{H} - \mathbf{W}\mathbf{X}||_F^{2} + \gamma ||\mathbf{W}||^2_F
    \label{eq:dl_nmf}
\end{equation}

matricea $\mathbf{H}$ reprezintă colecția de etichete pentru fiecare dintre reziduurile din $\mathbf{Y}$
forma lui fiind:

\begin{equation}
    \mathbf{H}_i = e_k, \text{dacă pentru exemplul i s-a produs un defect în nodul k}
    \label{eq:struct_H}
\end{equation}

parametrul $\gamma$ are rolul de a condiționa mai bine matricea și a evita fenomenul de \textit{overfitting}.
Matricea $\mathbf{W}$ reprezintă o matrice de transformare care în mod ideal încearcă să transforme vectorii rari din $\mathbf{X}$ în vectori asemănători versorilor $e_k \in \mathbb{R}^{n_{clase}}$, unde indexul variabilei nenule este corespunzător cu clasa defectului. Real vorbind, ceea ce se întâmplă este că rezultatul înmulțirii $\mathbf{W}\mathbf{X}$ este un vector dens, unde indexul celui mai mare element denotă de fapt clasa din care face parte reziduul transformat $p_{dl} = argmax(\mathbf{W}\mathbf{x})$ \cite[Capitolul 8]{DL_book}. 
Agregând cele două probleme de optimizare obținem învățarea de dicționare discriminative:

\begin{equation}
    \min_{\mathbf{D}, \mathbf{X}, \mathbf{W}} || \mathbf{Y} - \mathbf{D} \mathbf{X} ||^2_F + \alpha || \mathbf{H} - \mathbf{W} \mathbf{X} ||^2_F
    \label{eq:disc_DL}
\end{equation}


O altă modalitate de a spori performanțele de clasificare este prin obligarea problemei de optimizare să aloce anumiți atomi ai dicționarului $\mathbf{D}$ unor anumite clase de defecte - învățarea de dicționare cu consistență de clasă \textit{label consistent dictionary learning} \cite{DL_book}. Problema de optimizare la care se reduce este:

\begin{equation}
    \min_{\mb{D}, \mb{W}, \mb{A}, \mb{X}} || \mb{Y} - \mb{D}\mb{X}||^2_F + \alpha ||\mb{H} - \mb{W} \mb{X} ||^2_F + \beta ||\mb{Q} - \mb{A}\mb{X} ||^2_F
    \label{eq:lc_dl}
\end{equation}

matricea $\mb{Q}$ fiind de fapt parametrul care alocă fiecare atom unui anumit defect și contribuie la sporirea calității clasificării. Liniile matricei $\mb{Q}$ sunt asociate fiecărui atom cu proprietatea

\begin{equation}
    q_{ij} = \begin{cases}
    1,\quad \text{atomul i este asociat clasei j} \\
    0,\quad  \text{în rest}
    \end{cases}
\end{equation}

Problema \eqref{eq:lc_dl} poate fi redusă la problema de optimizare \cite{DL_book}:

\begin{equation}
    \min_{\mb{D}, \mb{W}, \mb{A}, \mb{X}} 
    \norm{
    \begin{bmatrix}
    \mb{Y} \\
    \sqrt{\alpha} \mb{H} \\
    \sqrt{\beta} \mb{Q}
    \end{bmatrix}
    -
     \begin{bmatrix}
    \mb{D} \\
    \sqrt{\alpha} \mb{W}\\
    \sqrt{\beta} \mb{A}
    \end{bmatrix} 
    \mb{X}
    }^2_F
    \label{eq:lc_dl}
\end{equation}

și poate fi rezolvată cu ajutorul algoritmului K-DVS care funcționează prin alternarea fixării matricelor $\mb{D}$ și $\mb{X}$, prima dată dicționarul este fixat și se găsește cea mai bună reprezentare $\mb{X}$, apoi se recalculează dicționarul $\mb{D}$. Metoda se bazează pe aproximarea unei matrice de rang n printr-o sumă de matrice de rang 1.





\section{Rezultate și metrici de clasificare}

Pentru testarea metodelor de DL am folosit setul de date de la capitolul \ref{chap:ml_classification}, iar metodele care calculează dicționarele și reprezentările sparse sunt cele de la \cite{dl-code}. Asemănător capitolului de clasificare cu SVM voi considera nodurile alese prin metoda MSC și prin metoda RFE, și voi arăta pentru fiecare dintre acestea acuratețea obținută cu ajutorul metodelor de DL cu consistență de clasă și DL discriminativ.
Astfel avem pentru senzorii selectați de algoritmul RFE:
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
         Nr senzori & Senzori selectați & Acuratețe DL discriminativ & Acuratețe LC-DL \\
        \hline

        4 & $10, 11, 25, 27$ & $46.5\%$ & $82.4\%$\\
        \hline
        6 & $10, 11, 16, 25, 27, 28$ & $53.1\%$ & $86.2\%$ \\
        \hline
        10 & $ 9, 10, 11, 12, 16, 20, 25, 27, 28, 29 $ & $57.2\%$ & $93.3\%$  \\
        \hline
    \end{tabular}
    \caption{Performanțele clasificării DL cu senzorii selectați de RFE}
    \label{tab:dl_rfe_performance}
\end{table}
Iar pentru metoda de plasare a senzorilor cu problema MSC am obținut rezultatele:

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
        Nr senzori & Senzori selectați & Acuratețe DL discriminativ & Acuratețe LC-DL \\
        \hline
        4 & $11, 15, 21, 28$ & $58.7\%$ & $89.7\%$ \\
        \hline
        6 & $12, 13, 16, 21, 25 , 26 $ & $51.4\%$ & $86.9\%$\\
        \hline
        10 & $6,12,13,14,15,16,21,26,27, 28$ & $65.59\%$ & $96.7\%$\\
        \hline
    \end{tabular}
    \caption{Performanțele clasificării DL cu senzorii selectați de MSC}
    \label{tab:dl_msc_performance}
\end{table}


Luând în calcul cei 10 senzori selectați de metoda MSC clasificatorul obținut de metoda învățarii de dicționare cu consistență de clase dă un rezultat al acurateții de 96.7\%, cu 2.3 \% mai performant decât abordarea cu SVM. Astfel putem afirma că învățare dicționarelor rare poate fi aplicată cu succes în problema clasificării defectelor într-o rețea cu apă și în principiu oferă o așa zisă robustețe, deoarece atomii asignați unei anumite clase vor reprezenta într-un mod prost profilurile defectelor din altă clasă.



